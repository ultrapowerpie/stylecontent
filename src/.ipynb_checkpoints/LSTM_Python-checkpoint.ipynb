{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Syllable-level LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Design notes:\n",
    "\n",
    "The class LSTM wraps a TF model (including hyperparameters, Variables and the computation graph).\n",
    "\n",
    "Non-TF computation (except feeding inputs) happens outside the class.\n",
    "\n",
    "Class methods preceeded by underscore (e.g. _init_params, _lstm_step) contain TF functions and are used to build the computation graphs for training and sampling. Placeholders are defined in `_build_graph`. These 'private' methods should be called within LSTM.\n",
    "\n",
    "Methods without underscore (`run_train`, `run_sample`) run a TF session and feed placeholder values but otherwise contain no TF functions. These 'public' methods should be called outside LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "\n",
    "    def __init__(self, batch_size, embedding_size, hidden_size, vocab_size, seq_length,\n",
    "                 learning_rate, decay_steps, decay_factor, sample_len, GPU=False):\n",
    "        ''' Set the hyperparameters and define the computation graph.\n",
    "        '''\n",
    "\n",
    "        ''' hyperparameters '''\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size # number of syllabs in vocab\n",
    "        self.seq_length = seq_length # number of steps to unroll the LSTM for\n",
    "        self.initial_learning_rate = learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_factor = decay_factor\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "        # this var keeps track of the train steps within the LSTM\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        ''' create vars and graph '''\n",
    "           \n",
    "        if GPU:\n",
    "            with tf.device(\"/gpu:0\"):\n",
    "                self._init_params()\n",
    "                self._build_graph()\n",
    "        else:\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                self._init_params()\n",
    "                self._build_graph()\n",
    "\n",
    "    def _init_params(self):\n",
    "        '''Create the model parameters'''\n",
    "        \n",
    "        # Learn an embedding for each syllable jointly with the other model params\n",
    "        self.embedding = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_size],\n",
    "                                                      mean=0, stddev=0.2))\n",
    "        self.Uf = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "        self.Ui = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "        self.Uo = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                       mean=0, stddev=0.2))\n",
    "        self.Uc = tf.Variable(tf.random_normal([self.hidden_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        self.Wf = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        self.Wi = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        self.Wo = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        self.Wc = tf.Variable(tf.random_normal([self.embedding_size, self.hidden_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        self.V = tf.Variable(tf.random_normal([self.hidden_size, self.vocab_size],\n",
    "                                               mean=0, stddev=0.2))\n",
    "        \n",
    "        self.bf = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "        self.bi = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "        self.bo = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "        self.bc = tf.Variable(tf.zeros([1, self.hidden_size]))\n",
    "        self.by = tf.Variable(tf.zeros([1, self.vocab_size]))\n",
    "\n",
    "    def _lstm_step(self, x, h, c):\n",
    "        '''Performs LSTM computation for one timestep:\n",
    "        takes a previous x and h, and computes the next x and h.\n",
    "        '''\n",
    "        \n",
    "        f = tf.nn.sigmoid(tf.matmul(x, self.Wf) + tf.matmul(h, self.Uf) + self.bf)\n",
    "        i = tf.nn.sigmoid(tf.matmul(x, self.Wi) + tf.matmul(h, self.Ui) + self.bi)\n",
    "        o = tf.nn.sigmoid(tf.matmul(x, self.Wo) + tf.matmul(h, self.Uo) + self.bo)\n",
    "        uc = tf.nn.tanh(tf.matmul(x, self.Wc) + tf.matmul(h, self.Uc) + self.bc)\n",
    "        c = tf.multiply(f, c)+tf.multiply(i, uc)\n",
    "        h = tf.multiply(o, tf.nn.tanh(c))\n",
    "        y = tf.matmul(h, self.V)+self.by\n",
    "\n",
    "        return y, h, c\n",
    "\n",
    "    \n",
    "    def _forward(self, inputs):\n",
    "        '''Performs the forward pass for all timesteps in a sequence.\n",
    "        '''\n",
    "        # Create list to hold y\n",
    "        y = [_ for _ in range(self.seq_length)]\n",
    "\n",
    "        # Create zero-d initial hidden state\n",
    "        h = tf.zeros([self.batch_size, self.hidden_size])\n",
    "        c = tf.zeros([self.batch_size, self.hidden_size])\n",
    "        \n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            x = tf.nn.embedding_lookup(self.embedding, inputs[:, t])\n",
    "            y[t], h, c = self._lstm_step(x, h, c)\n",
    "\n",
    "        return y\n",
    "\n",
    "    \n",
    "    def _sample_one(self, input_syllab, input_hidden, cell_state, temperature):\n",
    "        '''Sample the single next syllable in a sequence.'''\n",
    "\n",
    "        # We expand dims because tf expects a batch\n",
    "        syllab = tf.expand_dims(input_syllab, 0)\n",
    "\n",
    "        # Get the embedding for the input syllable\n",
    "        x = tf.nn.embedding_lookup(self.embedding, syllab)\n",
    "\n",
    "        # Take a single lstm step\n",
    "        y, h, c = self._lstm_step(x, input_hidden, cell_state)\n",
    "\n",
    "        # Dividing the unnormalized probabilities by the temperature before \n",
    "        # tf.multinomial is equivalent to adding temperature to a softmax\n",
    "        # before sampling\n",
    "        y_temperature = y / temperature\n",
    "\n",
    "        # We use tf.squeeze to remove the unnecessary [batch, num_samples] dims\n",
    "        # We do not manually softmax - tf.multinomial softmaxes the tensor we pass it\n",
    "        next_sample = tf.squeeze(tf.multinomial(y_temperature, 1))\n",
    "\n",
    "        return next_sample, h, c, y\n",
    "\n",
    "\n",
    "    def _build_graph(self):\n",
    "        '''Build the computation graphs for training and sampling.'''\n",
    "\n",
    "\n",
    "        '''Sampling and test graph'''\n",
    "        self.sample_input_syllab = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        self.sample_cell_state = tf.placeholder(dtype=tf.float32, shape=[1, self.hidden_size])\n",
    "        self.sample_input_hidden = tf.placeholder(dtype=tf.float32, shape=[1, self.hidden_size])\n",
    "        \n",
    "        self.test_syllab = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        \n",
    "        self.temperature = tf.placeholder_with_default(1.0, [])\n",
    "\n",
    "        self.next_sample, self.next_hidden, self.next_cell, self.next_predictions = self._sample_one(\n",
    "            self.sample_input_syllab, self.sample_input_hidden, self.sample_cell_state, self.temperature)\n",
    "        \n",
    "        self.next_softmax_predictions = tf.nn.softmax(self.next_predictions)\n",
    "                \n",
    "        self.test_syllab_prob = tf.reduce_sum(self.next_softmax_predictions * tf.one_hot(\n",
    "            tf.expand_dims(self.test_syllab, axis=0), depth=self.vocab_size))\n",
    "        \n",
    "        # Get cross entropy in base 2\n",
    "        # log_2 (x) =  log_e (x) / log_e(2)\n",
    "        self.binary_xentropy = - tf.log(self.test_syllab_prob) / tf.log(2.0)\n",
    "\n",
    "\n",
    "        '''Training graph'''\n",
    "        self.inputs = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.targets = tf.placeholder(dtype=tf.int32, shape=[None, self.seq_length])\n",
    "        self.predictions = self._forward(self.inputs)\n",
    "\n",
    "        cost_per_timestep_per_example = [\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.predictions[t],\n",
    "                    labels=self.targets[:, t])\n",
    "                for t in range(self.seq_length)\n",
    "        ]\n",
    "\n",
    "        # Use reduce_mean rather than reduce_sum over the examples in batch so that\n",
    "        # we don't need to change the learning rate when we change the batch size.\n",
    "        cost_per_timestep = [tf.reduce_mean(cost) for cost in cost_per_timestep_per_example]\n",
    "\n",
    "        # Use reduce_mean here too so we don't need to change the learning rate when\n",
    "        # we change number of timesteps.\n",
    "        self.cost = tf.reduce_mean(cost_per_timestep)\n",
    "\n",
    "        # Decay the learning rate according to a schedule.\n",
    "        self.learning_rate = tf.train.exponential_decay(self.initial_learning_rate,\n",
    "                                                        self.global_step,\n",
    "                                                        self.decay_steps,\n",
    "                                                        self.decay_factor)\n",
    "        \n",
    "        self.train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(\n",
    "            self.cost, global_step=self.global_step)\n",
    "\n",
    "\n",
    "        '''Finished creating graph: start session and init vars'''\n",
    "        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "        self.sess = tf.Session(config = config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def run_train(self, input_syllabs, target_syllabs):\n",
    "        '''Call this from outside the class to run a train step'''\n",
    "        cost, lr, _ = self.sess.run([self.cost, self.learning_rate, self.train_step],\n",
    "                                   feed_dict={\n",
    "                                       self.inputs: input_syllabs,\n",
    "                                       self.targets: target_syllabs\n",
    "                                   })\n",
    "        return cost, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_sample(model, n, starter_syllab, primer_seq=None, temperature=1.0):\n",
    "    '''Samples a length-n sequence from the model'''\n",
    "\n",
    "    sampled_syllabs = [_ for _ in range(n)]\n",
    "    current_syllab = starter_syllab\n",
    "    h = np.zeros([1, model.hidden_size])\n",
    "    cs = np.zeros([1, model.hidden_size])\n",
    "\n",
    "    if primer_seq is not None:\n",
    "        for c in primer_seq:\n",
    "            h, cs = model.sess.run(\n",
    "                [model.next_hidden, model.next_cell],\n",
    "                feed_dict={\n",
    "                    model.sample_input_syllab: c,\n",
    "                    model.sample_input_hidden: h,\n",
    "                    model.sample_cell_state: cs\n",
    "                })\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        current_syllab, h, cs = model.sess.run(\n",
    "            [model.next_sample, model.next_hidden, model.next_cell],\n",
    "            feed_dict={\n",
    "                model.sample_input_syllab: current_syllab,\n",
    "                model.sample_input_hidden: h,\n",
    "                model.sample_cell_state: cs,\n",
    "                model.temperature: temperature})\n",
    "\n",
    "        sampled_syllabs[i] = current_syllab\n",
    "\n",
    "    return sampled_syllabs\n",
    "\n",
    "def run_test(model, test_syllabs, primer_seq=None):\n",
    "    '''Finds the cross entropy on a dataset.\n",
    "    test_syllabs and primer_seq should be lists of ints.'''\n",
    "\n",
    "    xentropy_accum = 0.0\n",
    "    h = np.zeros([1, model.hidden_size])\n",
    "    cs = np.zeros([1, model.hidden_size])\n",
    "\n",
    "    if primer_seq is not None:\n",
    "        for c in primer_seq:\n",
    "            h, cs = model.sess.run(\n",
    "                [model.next_hidden, model.next_cell],\n",
    "                feed_dict={\n",
    "                    model.sample_input_syllab: c,\n",
    "                    model.sample_input_hidden: h,\n",
    "                    model.sample_cell_state: cs\n",
    "                })\n",
    "\n",
    "    for i in range(len(test_syllabs) - 1):\n",
    "        xentropy, h, cs  = model.sess.run(\n",
    "            [model.binary_xentropy, model.next_hidden, model.next_cell],\n",
    "            feed_dict={\n",
    "                model.sample_input_syllab: test_syllabs[i],\n",
    "                model.sample_input_hidden: h,\n",
    "                model.sample_cell_state: cs,\n",
    "                model.test_syllab: test_syllabs[i+1]\n",
    "            })\n",
    "\n",
    "        xentropy_accum += (xentropy / len(test_syllabs))\n",
    "\n",
    "    xentropy_avg = xentropy_accum \n",
    "\n",
    "    return xentropy_avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 28842 syllables, 41 unique.\n",
      "iter 0, loss: 3.900543, learning rate: 0.010000\n",
      "----\n",
      " ey g ae l f | jh z t ch ng t uh | | ay dh th w r ow n dh f p d | ow f  \n",
      " aa k aa aa w t aw ey z l b \n",
      " t er ih r aw r eh ay eh jh ae z ow dh th uh g g ah uh g uh n aa ey g ey z ch ah ih l ay t z g r  b \n",
      " t g b er y ah n jh aw uh aa g ow g p sh aw g ey z oy k eh d eh k ih z ow uh  | ih ih ow ih oy er uh th dh sh dh ah jh ah jh  uh | ng ch s l | ey sh iy ae t dh k g k ao t m f er l aw t dh f ow b ae \n",
      " k t \n",
      " ng hh t ah f ng jh jh iy iy er ih k hh b ch ng oy jh \n",
      " hh dh r ae | ae z hh z n jh ae sh iy z ng ao iy iy aw er ow aw ch ih p ch ey ng n eh | ch ng ao ao n m ay eh ng sh k  p l eh f b | dh ng er ch ch ch ng ch ng jh dh ch uw dh uw k th m jh th p  th jh |  m ch l aw y uh | ih  jh g ch uh m ah r oy r ow ow jh dh b aw b ey ah aw aw eh iy  w jh m er aa ey d uw iy |  | z ow ao eh | eh jh th ey hh eh l b oy ng \n",
      " oy l s th er ey er iy er oy th f aw th r sh w ay ao ey z jh z sh ah jh ah ae s dh er uh t z uw hh hh d er ih s t er z eh er f d ay d b iy ay w n f ay \n",
      " ah ch b th ng uw  l l uw w ch er er n k f ih ih t f uw w f l ng eh r f y n aw y hh m ow g ow iy y d aa aa ch n ng v ow r ch g ch v ae r y r sh z sh ng sh iy ch eh ey iy dh p th l uw ow ao f th th ao sh ey | ng f b ay l d aw \n",
      " y jh jh jh dh k m dh eh m v n \n",
      " ao l f dh t l v z z ey ng hh aw b r uh ey p z eh \n",
      "----\n",
      "iter 100, loss: 2.883055, learning rate: 0.009791\n",
      "iter 200, loss: 1.480983, learning rate: 0.009587\n",
      "iter 300, loss: 0.671314, learning rate: 0.009387\n",
      "iter 400, loss: 0.441438, learning rate: 0.009192\n",
      "iter 500, loss: 0.341689, learning rate: 0.009000\n",
      "iter 600, loss: 0.303012, learning rate: 0.008812\n",
      "iter 700, loss: 0.265718, learning rate: 0.008629\n",
      "iter 800, loss: 0.252932, learning rate: 0.008449\n",
      "iter 900, loss: 0.254693, learning rate: 0.008272\n",
      "iter 1000, loss: 0.239700, learning rate: 0.008100\n",
      "----\n",
      " z | d ah n | \n",
      " dh ae t | ih z | n aa t | ao l | \n",
      " l uh k | ae t | m iy | \n",
      " l uh k | ae t | dh eh m | \n",
      " s ae m | ay | ae m | \n",
      " k uh d | y uw | w uh d | y uw | \n",
      " w ih dh | ah | g ow t | \n",
      " ay | w uh d | n aa t | \n",
      " k uh d | n aa t | \n",
      " ih n | dh ah | d aa r k | \n",
      " hh iy r | ih n | dh ah | d aa r k | \n",
      " w uh d | y uw | k uh d | y uw | \n",
      " ih n | dh ah | r ey n | \n",
      " ay | w uh d | n aa t | k uh d | n aa t | ih n | dh ah | r ey n | \n",
      " ay | w ih l | n aa t | iy t | dh eh m | aa n | ah | t r ey n | \n",
      " n aa t | ih n | dh ah | d aa r k | n aa t | iy | s n d | ah n d | s ah m | aa r | g l ae d | \n",
      " ah n d | s ah m | aa r | g uh d | s t r iy t | \n",
      " w ih dh | dh ah | m aw s | ih n | dh ah | b aa k s | w ih dh | dh ah | hh uh k | \n",
      " ah n d | dh ah | k ae t | w eh n t | ah w ey | \n",
      " w ih dh | ah | s ae d | k ay n d | ah v | l uh k | \n",
      " dh ae t | ih z | g uh d | s eh d | dh ah | f ih sh | \n",
      " hh iy | hh ae z | g ao n | ah w ey | y eh s | \n",
      " b ah t | y ao r | m ah dh er | w ih l | k ah m | \n",
      " sh iy | w ih l | f ay n d | dh ih s | b ih g | m eh s | \n",
      " ah n d | w iy | s ao | s ah m | sh iy p | \n",
      " w iy | s ao | s ah m | sh iy p | t ey k | ah | w ao k | ih n | dh eh r | s t aa k | \n",
      " \n",
      "----\n",
      "iter 1100, loss: 0.244108, learning rate: 0.007931\n",
      "iter 1200, loss: 0.233364, learning rate: 0.007766\n",
      "iter 1300, loss: 0.225729, learning rate: 0.007604\n",
      "iter 1400, loss: 0.217198, learning rate: 0.007445\n",
      "iter 1500, loss: 0.213299, learning rate: 0.007290\n",
      "iter 1600, loss: 0.209196, learning rate: 0.007138\n",
      "iter 1700, loss: 0.212733, learning rate: 0.006989\n",
      "iter 1800, loss: 0.210317, learning rate: 0.006843\n",
      "iter 1900, loss: 0.207943, learning rate: 0.006701\n",
      "iter 2000, loss: 0.195629, learning rate: 0.006561\n",
      "----\n",
      " b iy t ah l z | b ae t ah l | b iy t ah l z | ih n | ah | p ah d ah l | p ae d ah l | b ae t ah l | \n",
      " ah n d | dh ah | b iy t ah l | b ae t ah l | p ah d ah l | p ae d ah l | b ae t ah l | \n",
      " ah n d | \n",
      " w eh n | b iy t ah l z | b ae t ah l | b iy t ah l z | ih n | ah | p ah d ah l | p ae d ah l | b ae t ah l | \n",
      " ah n d | \n",
      " w eh n | b iy t ah l z | b ae t ah l | b iy t ah l z | ih n | ah | p ah d ah l | p ae d ah l | b ae t ah l | \n",
      " ah n d | \n",
      " w eh n | b iy t ah l z | b ae t ah l | b iy t ah l z | ih n | ah | p ah d ah l | p ae d ah l | b ae t ah l | \n",
      " ah n d ow d | dh ah | hh ow m z | w eh r | dh ah | l ey | ih n | dh eh r | t aw n | \n",
      " ao l | dh eh r | w ih n d ow z | w er | d aa r k | k w ay ah t | s n ow | f ih l d | dh ah | eh r | \n",
      " ao l | dh ah | w er | ao l | d r iy m ih ng | s w iy t | d r iy m z | w ih th aw t | k eh r | \n",
      " w eh n | hh iy | k ey m | t uw | dh ah | f er s t | l ih t ah l | hh aw s | aa n | dh ah | s k w eh r | \n",
      " dh ih s | ih z | s t aa p | n ah m b er | w ah n | dh ah | ow l d | k l ao z | hh ih s t | \n",
      " ah n d | s ow | \n",
      " ay | w ih l | sh ow | y uw | ah n ah dh er | \n",
      " g uh d | t ae n t | \n",
      " w iy | hh ae v | t uw | f iy t | ah n \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "'''Train and sample from our model'''\n",
    "\n",
    "# data I/O\n",
    "corpus = open('../data/output/dr_seuss_phones.txt', 'r').read().split(\" \") # should be simple plain text file\n",
    "data = corpus#[:int(len(corpus)*0.9)]\n",
    "syllabs = list(set(data))\n",
    "data_size, vocab_size = len(data), len(syllabs)\n",
    "print 'data has %d syllables, %d unique.' % (data_size, vocab_size)\n",
    "syllab_to_ix = { s:i for i,s in enumerate(syllabs) }\n",
    "ix_to_syllab = { i:s for i,s in enumerate(syllabs) }\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "embedding_size = 32 # size of embedding\n",
    "hidden_size = 256 # size of hidden layers of neurons\n",
    "seq_length = 50 # number of steps to unroll the LSTM for\n",
    "learning_rate = 1e-2\n",
    "decay_steps = 500\n",
    "decay_factor = 0.9\n",
    "sample_len = 500\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "n_train_steps = 1\n",
    "\n",
    "# model parameters\n",
    "lstm = LSTM(batch_size, embedding_size, hidden_size, vocab_size, \n",
    "          seq_length, learning_rate, decay_steps, decay_factor, \n",
    "          sample_len, GPU=True)\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "for n in range(n_train_steps):\n",
    "    \n",
    "    # prepare inputs \n",
    "    inputs = np.empty([batch_size, seq_length])\n",
    "    targets = np.empty([batch_size, seq_length])\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # randomly index into the data for each example in batch\n",
    "        random_index = int(np.random.rand() * (data_size - seq_length - 1))\n",
    "        inputs[i, :] = [syllab_to_ix[ch] for ch in data[random_index:random_index+seq_length]]\n",
    "        targets[i, :] = [syllab_to_ix[ch] for ch in data[random_index+1:random_index+seq_length+1]]\n",
    "        \n",
    "    loss, lr = lstm.run_train(inputs, targets)\n",
    "    \n",
    "    # print progress\n",
    "    if n % 100 == 0: \n",
    "        print 'iter %d, loss: %f, learning rate: %f' % (n, loss, lr) \n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = lstm.run_sample(sample_len, inputs[0, 0], 1.0)\n",
    "        txt = ' '.join(ix_to_syllab[ix] for ix in sample_ix)\n",
    "        print '----\\n %s \\n----' % (txt, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2969801608087677"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cross-entropy on test set\n",
    "test = corpus[int(len(corpus)*0.9):]\n",
    "primer = data[-1000:]\n",
    "lstm.run_test([syllab_to_ix[ch] for ch in test], [syllab_to_ix[ch] for ch in primer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'zh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-e21d400a6a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/output/article3_phones.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meminem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msyllab_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprimer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix_to_syllab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'zh'"
     ]
    }
   ],
   "source": [
    "primer = open('../data/output/article3_phones.txt', 'r').read().split(\" \")\n",
    "sample_ix = run_sample(eminem, 10*sample_len, inputs[0, 0], [syllab_to_ix[ch] for ch in primer], 1)\n",
    "txt = ' '.join(ix_to_syllab[ix] for ix in sample_ix)\n",
    "print '%s' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling with high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zseyuwngshayeyahzhaeahaodhkbaafehmerhhiywsaheherehoyhh|hhaeae|iyoytawngkersthaomfihiychaezhhhae\n",
      "puherjhzhbeyehsgiyerereruhaomwhhoylwjheychyayddhaollynaapaozviheyeroyb\n",
      "ptheriyfdheyerbgfg\n",
      "biyhhuhaoehlowgayeyykbyayngzhphhaolwthhhzhuwndhoyshow\n",
      "vmuwnawzhwznihahowerzhfhh|roykuhsmytkhhuwrnaaehwnszhtfawpngshthshzhslshowfuhshrgawng\n",
      "nngfaydhzhdlgehwiyersheyjhuwzhah|shayuwowchkthmah\n",
      "oythl|vwaeeytdhawuwngaw\n",
      "hhbvffihiydherahzhiyihchthahvhhaewbowldhoyoyaobchteyzhuwlzeyihzaodhaemsfkahfchnerpjhayahayuhehhhowih\n",
      "sahehowshrdheyeraardch\n",
      "eyawchaojhthlowlnghhuwchihereymayaeaobgzihz\n",
      "zh||ihgbvnglehihlshehaeziyerfuh\n",
      "ayuhowzkng|chl\n",
      "faozdhhnglpaengwhherrkuwaommwzheythruhuhaoihhhfzheyhhaapngrnpeydaafluhhhzhhhsshtuhaetehshtd|pgkwwuhpmert\n",
      "mpjhvehverslzhaaweyoyshahfmdfjh|\n",
      "bchvdhaymchoy|sht|ngdherzs\n"
     ]
    }
   ],
   "source": [
    "sample_ix = lstm.run_sample(sample_len, inputs[0, 0], 100)\n",
    "txt = ''.join(ix_to_syllab[ix] for ix in sample_ix)\n",
    "print '%s' % (txt, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling with low temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he's inherited anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything anything any \n"
     ]
    }
   ],
   "source": [
    "sample_ix = lstm.run_sample(sample_len, inputs[0, 0], 0.001)\n",
    "txt = ' '.join(ix_to_syllab[ix] for ix in sample_ix)\n",
    "print '%s' % to_wordst(txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a soldier i'm living in a vibe let's go back \n",
    "throw your back then crazy \n",
    "ok when i'm one around about my \n",
    "switch it again and saran get to much to lose your man \n",
    "you don't know the crazy first \n",
    "\n",
    "as a people that i got your name \n",
    "i came and word though \n",
    "but cops there stopping in the worn of my skin is your forever dead dreams \n",
    "and remember me \n",
    "\n",
    "my darling your going from bagdad \n",
    "this time i'm bad \n",
    "seize all about to kill everywhere every sense \n",
    "and i won't be ran \n",
    "and care a \n",
    "my mother says "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i imagine death's door when we were needy \n",
    "we made a promise we signed a treaty \n",
    "i may have a friend \n",
    "chance to survive this \n",
    "stay alive high \n",
    "running out of time \n",
    "\n",
    "day you fight like you're \n",
    "running out of time \n",
    "day you fight like you're \n",
    "out your gun \n",
    "they surround our troops \n",
    "when they surround our troops \n",
    "they surround our in tolle than in new york you can \n",
    "\n",
    "even before we got to take a stand with the stamina god has gone gray he passes every day \n",
    "they say he walks the length of the plan was to write a total of twenty five essays\n",
    "you and your words flooded my senses \n",
    "your sentences left me defenseless "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he's never be president now \n",
    "that's one less thing to worry about \n",
    "where is it uptown \n",
    "and he just destroyed president \n",
    "weapon \n",
    "\n",
    "you'll be back time will tell \n",
    "you'll remember that i served you well \n",
    "boy your debts someone load his reputation \n",
    "welcome the cabinet i am in this show "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
